# è¯¥æ–‡ä»¶çš„åŠŸèƒ½å’Œtime_series_gca.pyç›¸åŒï¼Œ
# åŒºåˆ«åœ¨äºè¯¥æ–‡ä»¶æ˜¯tripe_gançš„å®ç°ï¼Œè€Œtime_series_gca.pyæ˜¯GCAçš„å®ç°ã€‚

"""
===============================================================================
åŠŸèƒ½æ¦‚è¿°:
æœ¬æ–‡ä»¶å®ç°äº†ä¸€ä¸ªåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ—¶é—´åºåˆ—æ¨¡å‹ï¼Œä¸“æ³¨äºä¸‰é‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆTriple GANï¼‰æ¶æ„çš„è®­ç»ƒä¸é¢„æµ‹ã€‚ä¸ä¼ ç»Ÿçš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç›¸æ¯”ï¼ŒTriple GAN å¼•å…¥äº†å¤šä¸ªç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æ¨¡å‹ï¼Œä»¥å¢å¼ºç”Ÿæˆå’Œåˆ†ç±»çš„èƒ½åŠ›ã€‚æœ¬æ–‡ä»¶ç»§æ‰¿äº† `GCABase` ç±»ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ‰©å±•ï¼Œå®ç°äº†æ—¶é—´åºåˆ—æ•°æ®çš„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€é¢„æµ‹ã€ä¿å­˜ä¸åŠ è½½ç­‰åŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½:
1. **è¶…å‚æ•°åˆå§‹åŒ–**:
   - `__init__()` æ–¹æ³•åˆå§‹åŒ–äº† Triple GAN æ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰è¶…å‚æ•°ï¼ŒåŒ…æ‹¬ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„æ•°é‡ã€æ‰¹é‡å¤§å°ã€è®­ç»ƒå‘¨æœŸæ•°ã€å­¦ä¹ ç‡ã€æ•°æ®è·¯å¾„ç­‰ã€‚
   - è‡ªåŠ¨è®¾ç½®è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰å¹¶ä¿è¯å®éªŒçš„å¯å¤ç°æ€§ã€‚

2. **æ•°æ®é¢„å¤„ç†**:
   - `process_data()` æ–¹æ³•ç”¨äºåŠ è½½ã€æ¸…æ´—å’Œåˆ’åˆ†è¾“å…¥æ•°æ®ã€‚å®ƒé€šè¿‡æ»‘åŠ¨çª—å£çš„æ–¹å¼å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼Œå¹¶å½’ä¸€åŒ–ç‰¹å¾å’Œç›®æ ‡å˜é‡ã€‚

3. **æ¨¡å‹åˆå§‹åŒ–**:
   - `init_model()` æ–¹æ³•åˆå§‹åŒ–ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æ¨¡å‹ï¼Œæ”¯æŒä¸åŒç±»å‹çš„ç”Ÿæˆå™¨ï¼ˆå¦‚ `Transformer`ã€`GRU`ï¼‰å’Œåˆ¤åˆ«å™¨ã€‚
   - æ”¯æŒæ ¹æ®é…ç½®åŠ¨æ€é€‰æ‹©ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„ç±»å‹ã€‚

4. **è®­ç»ƒä¸é¢„æµ‹**:
   - `train()` æ–¹æ³•è´Ÿè´£æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹ï¼Œä½¿ç”¨å¤šé‡ GAN ç»“æ„è¿›è¡Œè®­ç»ƒï¼Œä¼˜åŒ–ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„å‚æ•°ã€‚
   - `pred()` æ–¹æ³•åˆ™ç”¨äºåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ï¼Œè¿”å›æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚

5. **æ¨¡å‹ä¿å­˜ä¸åŠ è½½**:
   - `save_models()` æ–¹æ³•å°†è®­ç»ƒåçš„ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šçš„è·¯å¾„ï¼Œæ”¯æŒä»¥æ—¶é—´æˆ³å‘½åä¿å­˜æ–‡ä»¶ã€‚
   - `load_model()` æ–¹æ³•ä»æŒ‡å®šçš„æ£€æŸ¥ç‚¹åŠ è½½é¢„è®­ç»ƒçš„ç”Ÿæˆå™¨æ¨¡å‹ã€‚

6. **çŸ¥è¯†è’¸é¦ä¸è¯„ä¼°**:
   - `distill()` å’Œ `visualize_and_evaluate()` æ–¹æ³•ç”¨äºå®ç°æ¨¡å‹è’¸é¦å’Œå¯è§†åŒ–è¯„ä¼°ï¼Œå¸®åŠ©åˆ†æè®­ç»ƒç»“æœã€‚
   - `evaluate_best_models()` æ–¹æ³•ç”¨äºè¯„ä¼°æœ€ä¼˜æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ç”Ÿæˆè¯„ä¼°ç»“æœã€‚

7. **æ—¥å¿—è®°å½•ä¸æ—¶é—´ç›‘æ§**:
   - ä½¿ç”¨è£…é¥°å™¨ `log_execution_time()` æ¥è®°å½•æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹è¿‡ç¨‹ä¸­æ¯ä¸ªæ–¹æ³•çš„æ‰§è¡Œæ—¶é—´ï¼Œå¸®åŠ©ä¼˜åŒ–å®éªŒæ•ˆç‡ã€‚

ä½¿ç”¨æ–¹å¼:
- è¯¥æ–‡ä»¶æ˜¯ä¸€ä¸ªå®Œæ•´çš„ Triple GAN æ¨¡å‹å®ç°ï¼Œå¯ä»¥é€šè¿‡ `train()` æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œæˆ–é€šè¿‡ `pred()` æ–¹æ³•è¿›è¡Œé¢„æµ‹ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œä¼ å…¥è¶…å‚æ•°æ¥çµæ´»é…ç½®å®éªŒã€‚
- è®­ç»ƒç»“æœå°†è¢«ä¿å­˜åˆ°æŒ‡å®šç›®å½•ï¼Œä¸”æ”¯æŒåç»­åŠ è½½å’Œè¯„ä¼°æ¨¡å‹ã€‚
===============================================================================
"""

from GCA_base import GCABase
import torch
import numpy as np
from functools import wraps
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import TensorDataset, DataLoader
from utils.multiGAN_trainer_disccls import train_multi_gan
from typing import List, Optional
import models
import os
import time
import glob
from utils.evaluate_visualization import evaluate_best_models


def log_execution_time(func):
    """è£…é¥°å™¨ï¼šè®°å½•å‡½æ•°çš„è¿è¡Œæ—¶é—´ï¼Œå¹¶åŠ¨æ€è·å–å‡½æ•°å"""
    @wraps(func)  # ä¿ç•™åŸå‡½æ•°çš„å…ƒä¿¡æ¯ï¼ˆå¦‚ __name__ï¼‰
    def wrapper(*args, **kwargs):
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        result = func(*args, **kwargs)  # æ‰§è¡Œç›®æ ‡å‡½æ•°
        end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
        elapsed_time = end_time - start_time  # è®¡ç®—è€—æ—¶

        # åŠ¨æ€è·å–å‡½æ•°åï¼ˆæ”¯æŒç±»æ–¹æ³•å’Œæ™®é€šå‡½æ•°ï¼‰
        func_name = func.__name__
        print(f"GCA_time_series - '{func_name}' elapse time: {elapsed_time:.4f} sec")
        return result

    return wrapper


def generate_labels(y):
    """
    æ ¹æ®æ¯ä¸ªæ—¶é—´æ­¥ y æ˜¯å¦æ¯”å‰ä¸€æ—¶åˆ»æ›´é«˜ï¼Œç”Ÿæˆä¸‰åˆ†ç±»æ ‡ç­¾ï¼š
      - 2: å½“å‰å€¼ > å‰ä¸€æ—¶åˆ»ï¼ˆä¸Šå‡ï¼‰
      - 0: å½“å‰å€¼ < å‰ä¸€æ—¶åˆ»ï¼ˆä¸‹é™ï¼‰
      - 1: å½“å‰å€¼ == å‰ä¸€æ—¶åˆ»ï¼ˆå¹³ç¨³ï¼‰
    å¯¹äºç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œé»˜è®¤èµ‹å€¼ä¸º1ï¼ˆå¹³ç¨³ï¼‰ã€‚

    å‚æ•°ï¼š
        y: æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (æ ·æœ¬æ•°, ) æˆ– (æ ·æœ¬æ•°, 1)
    è¿”å›ï¼š
        labels: ç”Ÿæˆçš„æ ‡ç­¾æ•°ç»„ï¼Œé•¿åº¦ä¸ y ç›¸åŒ
    """
    y = np.array(y).flatten()  # è½¬æˆä¸€ç»´æ•°ç»„
    labels = [0]  # å¯¹äºç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œé»˜è®¤å¹³ç¨³
    for i in range(1, len(y)):
        if y[i] > y[i - 1]:
            labels.append(2)
        elif y[i] < y[i - 1]:
            labels.append(0)
        else:
            labels.append(1)
    return np.array(labels)


class GCA_time_series(GCABase):
    def __init__(self, args, N_pairs: int, batch_size: int, num_epochs: int,
                 generators_names: List, discriminators_names: Optional[List],
                 ckpt_dir: str, output_dir: str,
                 window_sizes: int,
                 initial_learning_rate: float = 2e-5,
                 train_split: float = 0.8,
                 do_distill_epochs: int = 1,
                 cross_finetune_epochs: int = 5,
                 precise=torch.float32,
                 device=None,
                 seed: int = None,
                 ckpt_path: str = None,
                 gan_weights=None,
                 ):
        """
        åˆå§‹åŒ–å¿…å¤‡çš„è¶…å‚æ•°ã€‚

        :param N_pairs: ç”Ÿæˆå™¨orå¯¹æŠ—å™¨çš„ä¸ªæ•°
        :param batch_size: å°æ‰¹æ¬¡å¤„ç†
        :param num_epochs: é¢„å®šè®­ç»ƒè½®æ•°
        :param initial_learning_rate: åˆå§‹å­¦ä¹ ç‡
        :param generators_names: list objectï¼ŒåŒ…æ‹¬äº†è¡¨ç¤ºå…·æœ‰ä¸åŒç‰¹å¾çš„ç”Ÿæˆå™¨çš„åç§°
        :param discriminators_names: list objectï¼ŒåŒ…æ‹¬äº†è¡¨ç¤ºå…·æœ‰ä¸åŒåˆ¤åˆ«å™¨çš„åç§°ï¼Œå¦‚æœæ²¡æœ‰å°±ä¸å†™é»˜è®¤ä¸€è‡´
        :param ckpt_dir: å„æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
        :param output_path: å¯è§†åŒ–ã€æŸå¤±å‡½æ•°çš„logç­‰è¾“å‡ºç›®å½•
        :param ckpt_path: é¢„æµ‹æ—¶ä¿å­˜çš„æ£€æŸ¥ç‚¹
        """
        super().__init__(N_pairs, batch_size, num_epochs,
                         generators_names, discriminators_names,
                         ckpt_dir, output_dir,
                         initial_learning_rate,
                         train_split,
                         precise,
                         do_distill_epochs, cross_finetune_epochs,
                         device,
                         seed,
                         ckpt_path)  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–

        self.args = args
        self.window_sizes = window_sizes
        # åˆå§‹åŒ–ç©ºå­—å…¸
        self.generator_dict = {}
        self.discriminator_dict = {"default": models.Discriminator3}

        # éå† model æ¨¡å—ä¸‹çš„æ‰€æœ‰å±æ€§
        for name in dir(models):
            obj = getattr(models, name)
            if isinstance(obj, type) and issubclass(obj, torch.nn.Module):
                lname = name.lower()
                if "generator" in lname:
                    key = lname.replace("generator_", "")
                    self.generator_dict[key] = obj
                elif "discriminator" in lname:
                    key = lname.replace("discriminator", "")
                    self.discriminator_dict[key] = obj

        self.gan_weights = gan_weights
        self.init_hyperparameters()

    @log_execution_time
    def process_data(self, data_path, start_row, end_row,  target_columns, feature_columns_list):
        """
        å¤„ç†è¾“å…¥æ•°æ®ï¼ŒåŒ…æ‹¬åŠ è½½ã€æ‹†åˆ†å’Œå½’ä¸€åŒ–ã€‚

        å‚æ•°:
            data_path (str): CSV æ•°æ®æ–‡ä»¶çš„è·¯å¾„
            target_columns (list): ç›®æ ‡åˆ—çš„ç´¢å¼•
            feature_columns (list): ç‰¹å¾åˆ—çš„ç´¢å¼•

        è¿”å›:
            tuple: (train_x, test_x, train_y, test_y, y_scaler)
        """
        print(f"Processing data with seed: {self.seed}")  # Using self.seed

        # è½½å…¥æ•°æ®
        data = pd.read_csv(data_path)

        # åˆ‡ç‰‡ç›®æ ‡æ•°æ®
        y = data.iloc[start_row:end_row, target_columns].values
        target_column_names = data.columns[target_columns]
        print("Target columns:", target_column_names)

        # åˆ‡ç‰‡ç‰¹å¾
        x_list = []
        feature_column_names_list = []
        self.x_scalers = []  # Store multiple x scalers

        for feature_columns in feature_columns_list:
            # Select feature columns
            x = data.iloc[start_row:end_row, feature_columns].values
            feature_column_names = data.columns[feature_columns]
            print("Feature columns:", feature_column_names)

            x_list.append(x)
            feature_column_names_list.append(feature_column_names)

        # åˆ‡ç‰‡è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œ3ï¼š7
        train_size = int(data.iloc[start_row:end_row].shape[0] * self.train_split)
        train_x_list = [x[:train_size] for x in x_list]
        test_x_list = [x[train_size:] for x in x_list]
        train_y, test_y = y[:train_size], y[train_size:]


        self.train_x_list = []
        self.test_x_list = []

        for train_x, test_x in zip(train_x_list, test_x_list):
            x_scaler = MinMaxScaler(feature_range=(0, 1))
            self.train_x_list.append(x_scaler.fit_transform(train_x))
            self.test_x_list.append(x_scaler.transform(test_x))
            self.x_scalers.append(x_scaler)

        #  å½’ä¸€åŒ–
        """
        fit_transform æ–¹æ³•ä¼šè®¡ç®— train_y çš„æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯å­˜å‚¨åœ¨ self.y_scaler ä¸­ã€‚
        ç„¶åï¼Œå®ƒä¼šå°† train_y ç¼©æ”¾åˆ° [0, 1] çš„èŒƒå›´ï¼Œå¹¶å°†ç»“æœèµ‹å€¼ç»™ self.train_yã€‚
        """
        self.x_scaler = MinMaxScaler(feature_range=(0, 1))  # Store scaler as instance variable
        self.y_scaler = MinMaxScaler(feature_range=(0, 1))  # Store scaler as instance variable

        self.train_y = self.y_scaler.fit_transform(train_y)
        self.test_y = self.y_scaler.transform(test_y)

        # ç”Ÿæˆè®­ç»ƒé›†çš„åˆ†ç±»æ ‡ç­¾ï¼ˆç›´æ¥åœ¨ GPU ä¸Šç”Ÿæˆï¼‰
        self.train_labels = generate_labels(self.train_y)
        # ç”Ÿæˆæµ‹è¯•é›†çš„åˆ†ç±»æ ‡ç­¾
        self.test_labels = generate_labels(self.test_y)
        print(self.train_y[:5])
        print(self.train_labels[:5])


    def create_sequences_combine(self, x_list, y, label, window_size, start):

        # åˆå§‹åŒ–
        x_ = []
        y_ = []
        y_gan = []
        label_gan = []

        # åˆ©ç”¨çª—å£æ»‘åŠ¨å¤„ç†æ•°æ®
        # çª—å£å¤§å°ä¸º window_sizeï¼Œä» start å¼€å§‹æ»‘åŠ¨
        for x in x_list:
            x_seq = []
            for i in range(start, x.shape[0]):
                tmp_x = x[i - window_size: i, :]
                x_seq.append(tmp_x)
            x_.append(np.array(x_seq))

        # Combine x sequences along feature dimension
        x_ = np.concatenate(x_, axis=-1)

        for i in range(start, y.shape[0]):
            tmp_y = y[i]
            tmp_y_gan = y[i - window_size: i + 1]
            tmp_label_gan = label[i - window_size: i + 1]

            y_.append(tmp_y)
            y_gan.append(tmp_y_gan)
            label_gan.append(tmp_label_gan)

        # æ•°æ®è½¬æˆ tensor
        x_ = torch.from_numpy(np.array(x_)).float()
        y_ = torch.from_numpy(np.array(y_)).float()
        y_gan = torch.from_numpy(np.array(y_gan)).float()
        label_gan = torch.from_numpy(np.array(label_gan)).float()
        return x_, y_, y_gan, label_gan

    @log_execution_time
    def init_dataloader(self):
        """åˆå§‹åŒ–ç”¨äºè®­ç»ƒä¸è¯„ä¼°çš„æ•°æ®åŠ è½½å™¨"""

        # åˆ©ç”¨create_sequences_combineæ„é€ è®­ç»ƒé›†ã€æµ‹è¯•é›†æ•°æ®
        train_data_list = [
            self.create_sequences_combine(self.train_x_list, self.train_y, self.train_labels, w, self.window_sizes[-1])
            for w in self.window_sizes
        ]
        print()
        test_data_list = [
            self.create_sequences_combine(self.test_x_list, self.test_y, self.test_labels, w, self.window_sizes[-1])
            for w in self.window_sizes
        ]

        # ä» train_data_list å’Œ test_data_list ä¸­æå–è¾“å…¥ç‰¹å¾ã€ç›®æ ‡å€¼ã€ç”Ÿæˆå™¨è®­ç»ƒæ‰€éœ€çš„ç›®æ ‡åºåˆ—å’Œæ ‡ç­¾æ•°æ®ï¼Œå¹¶å°†å®ƒä»¬è½¬ç§»åˆ°æŒ‡å®šçš„è®¾å¤‡
        self.train_x_all = [x.to(self.device) for x, _, _, _ in train_data_list]
        self.train_y_all = train_data_list[0][1]  # æ‰€æœ‰ y åº”è¯¥ç›¸åŒï¼Œå–ç¬¬ä¸€ä¸ªå³å¯ï¼Œä¸ç”¨cudaå› ä¸ºè¦eval
        self.train_y_gan_all = [y_gan.to(self.device) for _, _, y_gan, _ in train_data_list]
        self.train_label_gan_all = [label_gan.to(self.device) for _, _, _, label_gan in train_data_list]

        self.test_x_all = [x.to(self.device) for x, _, _, _ in test_data_list]
        self.test_y_all = test_data_list[0][1]  # æ‰€æœ‰ y åº”è¯¥ç›¸åŒï¼Œå–ç¬¬ä¸€ä¸ªå³å¯ï¼Œä¸ç”¨cudaå› ä¸ºè¦eval
        self.test_y_gan_all = [y_gan.to(self.device) for _, _, y_gan, _ in test_data_list]
        self.test_label_gan_all = [label_gan.to(self.device) for _, _, _, label_gan in test_data_list]

        # ä» train_data_list å’Œ test_data_list ä¸­æå–è¾“å…¥ç‰¹å¾ã€ç›®æ ‡å€¼ã€ç”Ÿæˆå™¨è®­ç»ƒæ‰€éœ€çš„ç›®æ ‡åºåˆ—å’Œæ ‡ç­¾æ•°æ®ï¼Œå¹¶å°†å®ƒä»¬è½¬ç§»åˆ°æŒ‡å®šçš„è®¾å¤‡
        assert all(torch.equal(train_data_list[0][1], y) for _, y, _, _ in train_data_list), "Train y mismatch!"
        assert all(torch.equal(test_data_list[0][1], y) for _, y, _, _ in test_data_list), "Test y mismatch!"

        """
        train_x_all.shape  # (N, N, W, F)  ä¸åŒ window_size ä¼šå¯¼è‡´ W ä¸ä¸€æ ·ï¼Œåªèƒ½åœ¨ W ç›¸åŒæ—¶ç”¨ stack
        train_y_all.shape  # (N,)
        train_y_gan_all.shape  # (3, N, W+1)
        """

        self.dataloaders = []
        # éå†ä¸åŒ window_size çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæ„é€  DataLoader
        for i, (x, y_gan, label_gan) in enumerate(
                zip(self.train_x_all, self.train_y_gan_all, self.train_label_gan_all)):
            shuffle_flag = ("transformer" in self.generator_names[i])  # æœ€åä¸€ä¸ªè®¾ç½®ä¸º shuffle=Trueï¼Œå…¶ä½™ä¸º False
            dataloader = DataLoader(
                TensorDataset(x, y_gan, label_gan),
                batch_size=self.batch_size,
                shuffle=shuffle_flag,
                generator=torch.manual_seed(self.seed),
                drop_last=True  # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸è¶³ batch size çš„æ•°æ®
            )
            self.dataloaders.append(dataloader)

    def init_model(self,num_cls):
        """æ¨¡å‹ç»“æ„åˆå§‹åŒ–"""

        # æ£€æŸ¥ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¸€è‡´æ€§
        assert len(self.generator_names) == self.N, "Generators and Discriminators mismatch!"
        assert isinstance(self.generator_names, list)
        for i in range(self.N):
            assert isinstance(self.generator_names[i], str)

        self.generators = []
        self.discriminators = []

        for i, name in enumerate(self.generator_names):
            # è·å–å¯¹åº”çš„ x, y
            x = self.train_x_all[i]
            y = self.train_y_all[i]

            # åˆå§‹åŒ–ç”Ÿæˆå™¨
            GenClass = self.generator_dict[name]
            if "transformer" in name:
                gen_model = GenClass(x.shape[-1], output_len=y.shape[-1]).to(self.device)
            else:
                gen_model = GenClass(x.shape[-1], y.shape[-1]).to(self.device)

            self.generators.append(gen_model)

            # åˆå§‹åŒ–åˆ¤åˆ«å™¨ï¼ˆé»˜è®¤åªç”¨ Discriminator3ï¼‰
            DisClass = self.discriminator_dict[
                "default" if self.discriminators_names is None else self.discriminators_names[i]]
            dis_model = DisClass(self.window_sizes[i], out_size=y.shape[-1], num_cls=num_cls).to(self.device)
            self.discriminators.append(dis_model)

    def init_hyperparameters(self, ):
        """åˆå§‹åŒ–è®­ç»ƒæ‰€éœ€çš„è¶…å‚æ•°"""

        # åˆå§‹åŒ– init_GDweight æƒé‡çŸ©é˜µã€‚åˆå§‹åŒ–ï¼šå¯¹è§’çº¿ä¸Šä¸º1ï¼Œå…¶ä½™ä¸º0ï¼Œæœ€åä¸€åˆ—ä¸º1.0
        self.init_GDweight = []
        for i in range(self.N):
            row = [0.0] * self.N
            row[i] = 1.0
            row.append(1.0)  # æœ€åä¸€åˆ—ä¸º scale
            self.init_GDweight.append(row)

        if self.gan_weights is None:
            # æœ€ç»ˆï¼šå‡åˆ†ç»„åˆï¼Œæœ€åä¸€åˆ—ä¸º1.0
            final_row = [round(1.0 / self.N, 3)] * self.N + [1.0]
            self.final_GDweight = [final_row[:] for _ in range(self.N)]
        else:
            pass
        # åˆå§‹åŒ–å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨å‚æ•°
        self.g_learning_rate = self.initial_learning_rate
        self.d_learning_rate = self.initial_learning_rate

        # åˆå§‹åŒ–å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨å‚æ•°
        self.adam_beta1, self.adam_beta2 = (0.9, 0.999)

        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨çš„å‚æ•°
        self.schedular_factor = 0.1
        self.schedular_patience = 16
        self.schedular_min_lr = 1e-7

    def train(self, logger):
        results, best_model_state = train_multi_gan(self.args, self.generators, self.discriminators, self.dataloaders,
                                                    self.window_sizes,
                                                    self.y_scaler, self.train_x_all, self.train_y_all, self.test_x_all,
                                                    self.test_y_all, self.train_label_gan_all[0], self.test_label_gan_all[0],
                                                    self.do_distill_epochs,self.cross_finetune_epochs,
                                                    self.num_epochs,
                                                    self.output_dir,
                                                    self.device,
                                                    init_GDweight=self.init_GDweight,
                                                    final_GDweight=self.final_GDweight,
                                                    logger=logger)

        self.save_models(best_model_state)
        return results

    def save_models(self, best_model_state):
        """
        ä¿å­˜æ‰€æœ‰ generator å’Œ discriminator çš„æ¨¡å‹å‚æ•°ï¼ŒåŒ…å«æ—¶é—´æˆ³ã€æ¨¡å‹åç§°æˆ–ç¼–å·ã€‚
        """
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        ckpt_dir = os.path.join(self.ckpt_dir, timestamp)
        gen_dir = os.path.join(ckpt_dir, "generators")
        disc_dir = os.path.join(ckpt_dir, "discriminators")
        os.makedirs(gen_dir, exist_ok=True)
        os.makedirs(disc_dir, exist_ok=True)

        # åŠ è½½æ¨¡å‹å¹¶è®¾ä¸º eval
        for i in range(self.N):
            self.generators[i].load_state_dict(best_model_state[i])
            self.generators[i].eval()

        for i, gen in enumerate(self.generators):
            gen_name = type(gen).__name__
            save_path = os.path.join(gen_dir, f"{i + 1}_{gen_name}.pt")
            torch.save(gen.state_dict(), save_path)

        for i, disc in enumerate(self.discriminators):
            disc_name = type(disc).__name__
            save_path = os.path.join(disc_dir, f"{i + 1}_{disc_name}.pt")
            torch.save(disc.state_dict(), save_path)

        print("All models saved with timestamp and identifier.")

    # å–æŒ‡å®šç›®å½•ä¸­æœ€æ–°çš„æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰æ–‡ä»¶å¤¹ã€‚
    def get_latest_ckpt_folder(self):
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        all_subdirs = [d for d in glob.glob(os.path.join(self.ckpt_dir, timestamp[0] + "*")) if os.path.isdir(d)]
        if not all_subdirs:
            raise FileNotFoundError("âŒ No checkpoint records!!")
        latest = max(all_subdirs, key=os.path.getmtime)
        print(f"ğŸ“‚ Auto loaded checkpoint file: {latest}")
        return latest

    # è£…è½½æ¨¡å‹
    def load_model(self):
        gen_path = os.path.join(self.ckpt_path, "g{gru}", "generator.pt")
        if os.path.exists(gen_path):
            self.generators[0].load_state_dict(torch.load(gen_path, map_location=self.device))
            print(f"âœ… Loaded generator from {gen_path}")
        else:
            raise FileNotFoundError(f"âŒ Generator checkpoint not found at: {gen_path}")

    def pred(self):
        if self.ckpt_path == "auto":
            self.ckpt_path = self.get_latest_ckpt_folder()

        print("Start predicting with all generators..")

        best_model_state = []
        ckpt_dir = os.path.normpath(self.ckpt_path)  # è§„èŒƒåŒ–è·¯å¾„
        generators_dir = os.path.join(ckpt_dir, "generators")

        # éå†æ‰€æœ‰ç”Ÿæˆå™¨
        for i in range(self.N):
            # åŠ¨æ€æ„é€ æ–‡ä»¶åï¼ˆæ ¼å¼ï¼šåºå·_ç”Ÿæˆå™¨ç±»å.ptï¼‰
            gen = self.generators[i]
            gen_name = type(gen).__name__  # è·å–ç”Ÿæˆå™¨ç±»åï¼Œå¦‚Generator_Transformer
            filename = f"{i + 1}_{gen_name}.pt"  # ç”Ÿæˆç±»ä¼¼ 1_Generator_Transformer.pt

            # æ„é€ å®Œæ•´æ–‡ä»¶è·¯å¾„
            model_path = os.path.join(generators_dir, filename)
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½ç”Ÿæˆå™¨ {i + 1}/{self.N}: {model_path}")

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

            # åŠ è½½æ¨¡å‹å‚æ•°
            try:
                state_dict = torch.load(model_path, map_location=self.device)
                gen.load_state_dict(state_dict)
                gen.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
                best_model_state.append(state_dict)
                print(f"âœ… æˆåŠŸåŠ è½½ç”Ÿæˆå™¨ {i + 1}ï¼š{gen_name}")
            except Exception as e:
                raise RuntimeError(f"âŒ åŠ è½½ç”Ÿæˆå™¨ {i + 1} å¤±è´¥: {str(e)}")

        results = evaluate_best_models(self.generators, best_model_state, self.train_x_all, self.train_y_all,
                                       self.test_x_all, self.test_y_all, self.y_scaler,
                                       self.output_dir)
        return results


    def distill(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶å¯è§†åŒ–ç»“æœ"""
        pass

    def visualize_and_evaluate(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶å¯è§†åŒ–ç»“æœ"""
        pass

    def init_history(self):
        """åˆå§‹åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡è®°å½•ç»“æ„"""
        pass